---
layout: post
title: Tricks of NN
date: 2017-08-18
categories: blog
tags: [ML]
description: 神经网络的各种招数
---

### LRN
似乎已被弃用，除了AlexNet，效果不太好。

### 数据增强
尤其对图像来说，可以做crops，旋转，还可以做multi-size的训练。

### Batch Normalization
对mini-batch的各样本同一维度进行处理，需要batch size，不适合直接用于RNN。

### Layer Normalization
对同一样本的各个维度进行处理，可以轻松用于RNN，Transformer

